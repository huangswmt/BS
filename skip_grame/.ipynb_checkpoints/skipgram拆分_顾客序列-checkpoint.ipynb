{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "tf.disable_v2_behavior()\n",
    "#import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_data_load(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def init_key(self,keypath):\n",
    "        workey=np.load(keypath,allow_pickle=True)\n",
    "        workey = list(set(workey))\n",
    "        int_to_vocab = {j:i for i,j in zip(workey,range(len(workey)))}\n",
    "        word2int = {int_to_vocab[i]:i for i in int_to_vocab}\n",
    "        \n",
    "        self.int_to_vocab = int_to_vocab\n",
    "        self.word2int = word2int\n",
    "        self.workey = workey\n",
    "    \n",
    "    def init_data(self,datapath):\n",
    "        alllist = np.load(datapath,allow_pickle=True)\n",
    "        self . alllist = alllist\n",
    "        \n",
    "    def trans_2_int(self):\n",
    "        alllist = self . alllist\n",
    "        train_wordslist = []\n",
    "        for wo in alllist:\n",
    "            train_words =[self.word2int[i] for i in wo if (i in self.word2int) ]\n",
    "            if len(train_words)>2:\n",
    "                train_wordslist.append(train_words)\n",
    "        self . train_wordslist = train_wordslist\n",
    "\n",
    "        \n",
    "    def get_target(self, words, idx, window_size=5):\n",
    "       # ''' Get a list of words in a window around an index. '''\n",
    "\n",
    "        R = np.random.randint(1, window_size+1)\n",
    "        #print(R)\n",
    "        start = idx - R if (idx - R) > 0 else 0\n",
    "        stop = idx + R\n",
    "        target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "\n",
    "        return list(target_words)    \n",
    "    \n",
    "    \n",
    "    def get_batches(self, batch_size, window_size=5):\n",
    "        ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "\n",
    "        #n_batches = len(words)//batch_size\n",
    "\n",
    "        # only full batches\n",
    "        #words = words[:n_batches*batch_size]\n",
    "        wordsl = self.train_wordslist\n",
    "        all_batch = []\n",
    "        for words in ( (wordsl)):\n",
    "            if len(words)>batch_size:\n",
    "                n_batches = len(words)//batch_size\n",
    "                words = words[:n_batches*batch_size]\n",
    "                for idx in range(0, len(words), batch_size):\n",
    "                    x, y = [], []\n",
    "                    batch = words[idx:idx+batch_size]\n",
    "                    for ii in range(len(batch)):\n",
    "                        batch_x = batch[ii]\n",
    "                        batch_y = self.get_target(batch, ii, window_size)\n",
    "                        y.extend(batch_y)\n",
    "                        x.extend([batch_x]*len(batch_y))\n",
    "                    yield x, y\n",
    "            else:\n",
    "                x, y = [], []\n",
    "                batch = words\n",
    "                for ii in range(len(batch)):\n",
    "                    batch_x = batch[ii]\n",
    "                    batch_y = self.get_target(batch, ii, window_size)\n",
    "                    y.extend(batch_y)\n",
    "                    x.extend([batch_x]*len(batch_y))\n",
    "\n",
    "                yield x, y\n",
    "                \n",
    "                \n",
    "    def train_s(self,n_embedding = 20,n_sampled = 5,epochs = 40,batch_size = 1000,window_size = 10,ch_save = \"checkpoints_close/text8.ckpt\"):\n",
    "    \n",
    "        loglist = []\n",
    "        \n",
    "        \n",
    "        valid_size = 5# Random set of words to evaluate similarity on.\n",
    "        valid_window = 200\n",
    "        \n",
    "        word2int = self.word2int\n",
    "        train_graph = tf.Graph()\n",
    "        with train_graph.as_default():\n",
    "            inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "            labels = tf.placeholder(tf.int32, [None, 1], name='labels')\n",
    "\n",
    "        n_vocab = len(word2int)\n",
    "        # Number of embedding features \n",
    "        with train_graph.as_default():\n",
    "            embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "            embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "        n_vocab\n",
    "\n",
    "        # Number of negative labels to sample\n",
    "\n",
    "        with train_graph.as_default():\n",
    "            softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "\n",
    "            # Calculate the loss using negative sampling\n",
    "            loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                              labels, embed,\n",
    "                                              n_sampled, n_vocab)\n",
    "\n",
    "            cost = tf.reduce_mean(loss)\n",
    "            optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "        with train_graph.as_default():\n",
    "            ## From Thushan Ganegedara's implementation\n",
    "\n",
    "            # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "            valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "            valid_examples = np.append(valid_examples, \n",
    "                                       random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "            valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "            # We use the cosine distance:\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "            normalized_embedding = embedding / norm\n",
    "            valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "            similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))\n",
    "\n",
    "        # If the checkpoints directory doesn't exist:\n",
    "        !mkdir checkpoints_close\n",
    "\n",
    "        with train_graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(graph=train_graph) as sess:\n",
    "            iteration = 1\n",
    "            loss = 0\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for e in range(1, epochs+1):\n",
    "                batches = Tdl.get_batches(batch_size)\n",
    "                start = time.time()\n",
    "                for x, y in batches:\n",
    "\n",
    "                    feed = {inputs: x,\n",
    "                            labels: np.array(y)[:, None]}\n",
    "                    train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                    loss += train_loss\n",
    "\n",
    "                    if iteration % 200 == 0: \n",
    "                        end = time.time()\n",
    "                        print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                              \"Iteration: {}\".format(iteration),\n",
    "                              \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                              \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                        loss = 0\n",
    "                        loglist.append(loss/100)\n",
    "                        start = time.time()\n",
    "\n",
    "                    if iteration % 400 == 0:\n",
    "                        try:\n",
    "                            # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                            sim = similarity.eval()\n",
    "                            for i in range(valid_size):\n",
    "                                valid_word = int_to_vocab[valid_examples[i]]\n",
    "                                top_k = 4 # number of nearest neighbors\n",
    "                                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                                log = 'Nearest to %s:' % valid_word\n",
    "                                for k in range(top_k):\n",
    "\n",
    "                                    close_word = int_to_vocab[nearest[k]]\n",
    "                                    log = '%s %s,' % (log, close_word)\n",
    "                                print(log)\n",
    "\n",
    "                        except:\n",
    "                            print('error')\n",
    "\n",
    "                    iteration += 1\n",
    "            save_path = saver.save(sess, ch_save)\n",
    "            embed_mat = sess.run(normalized_embedding)\n",
    "            self.embed_mat = embed_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypath = '../../毕设/joycity_shop_list3_key.npy'\n",
    "datapath  ='../../毕设/joycity_shop_list3.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdl = Train_data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdl.init_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdl.init_key(keypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tdl.trans_2_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints_close’: File exists\n",
      "Epoch 1/40 Iteration: 200 Avg. Training loss: 1.5086 0.0182 sec/batch\n",
      "Epoch 1/40 Iteration: 400 Avg. Training loss: 1.4877 0.0158 sec/batch\n",
      "error\n",
      "Epoch 1/40 Iteration: 600 Avg. Training loss: 1.5840 0.0157 sec/batch\n",
      "Epoch 2/40 Iteration: 800 Avg. Training loss: 1.6417 0.0003 sec/batch\n",
      "error\n",
      "Epoch 2/40 Iteration: 1000 Avg. Training loss: 1.4415 0.0160 sec/batch\n",
      "Epoch 2/40 Iteration: 1200 Avg. Training loss: 1.5117 0.0158 sec/batch\n",
      "error\n",
      "Epoch 2/40 Iteration: 1400 Avg. Training loss: 1.5170 0.0154 sec/batch\n",
      "Epoch 3/40 Iteration: 1600 Avg. Training loss: 1.4956 0.0007 sec/batch\n",
      "error\n",
      "Epoch 3/40 Iteration: 1800 Avg. Training loss: 1.5676 0.0160 sec/batch\n",
      "Epoch 3/40 Iteration: 2000 Avg. Training loss: 1.6417 0.0157 sec/batch\n",
      "error\n",
      "Epoch 3/40 Iteration: 2200 Avg. Training loss: 1.5134 0.0153 sec/batch\n",
      "Epoch 4/40 Iteration: 2400 Avg. Training loss: 1.5497 0.0011 sec/batch\n",
      "error\n",
      "Epoch 4/40 Iteration: 2600 Avg. Training loss: 1.5307 0.0160 sec/batch\n",
      "Epoch 4/40 Iteration: 2800 Avg. Training loss: 1.4189 0.0158 sec/batch\n",
      "error\n",
      "Epoch 4/40 Iteration: 3000 Avg. Training loss: 1.5136 0.0152 sec/batch\n",
      "Epoch 5/40 Iteration: 3200 Avg. Training loss: 1.5367 0.0015 sec/batch\n",
      "error\n",
      "Epoch 5/40 Iteration: 3400 Avg. Training loss: 1.4902 0.0160 sec/batch\n",
      "Epoch 5/40 Iteration: 3600 Avg. Training loss: 1.5062 0.0158 sec/batch\n",
      "error\n",
      "Epoch 5/40 Iteration: 3800 Avg. Training loss: 1.4447 0.0153 sec/batch\n",
      "Epoch 6/40 Iteration: 4000 Avg. Training loss: 1.4439 0.0018 sec/batch\n",
      "error\n",
      "Epoch 6/40 Iteration: 4200 Avg. Training loss: 1.3728 0.0159 sec/batch\n",
      "Epoch 6/40 Iteration: 4400 Avg. Training loss: 1.4022 0.0160 sec/batch\n",
      "error\n",
      "Epoch 6/40 Iteration: 4600 Avg. Training loss: 1.3786 0.0155 sec/batch\n",
      "Epoch 7/40 Iteration: 4800 Avg. Training loss: 1.5134 0.0023 sec/batch\n",
      "error\n",
      "Epoch 7/40 Iteration: 5000 Avg. Training loss: 1.4196 0.0160 sec/batch\n",
      "Epoch 7/40 Iteration: 5200 Avg. Training loss: 1.5036 0.0159 sec/batch\n",
      "error\n",
      "Epoch 7/40 Iteration: 5400 Avg. Training loss: 1.3489 0.0154 sec/batch\n",
      "Epoch 8/40 Iteration: 5600 Avg. Training loss: 1.3744 0.0027 sec/batch\n",
      "error\n",
      "Epoch 8/40 Iteration: 5800 Avg. Training loss: 1.4762 0.0159 sec/batch\n",
      "Epoch 8/40 Iteration: 6000 Avg. Training loss: 1.4138 0.0158 sec/batch\n",
      "error\n",
      "Epoch 8/40 Iteration: 6200 Avg. Training loss: 1.3138 0.0159 sec/batch\n",
      "Epoch 9/40 Iteration: 6400 Avg. Training loss: 1.3933 0.0031 sec/batch\n",
      "error\n",
      "Epoch 9/40 Iteration: 6600 Avg. Training loss: 1.4540 0.0158 sec/batch\n",
      "Epoch 9/40 Iteration: 6800 Avg. Training loss: 1.4196 0.0159 sec/batch\n",
      "error\n",
      "Epoch 9/40 Iteration: 7000 Avg. Training loss: 1.4172 0.0153 sec/batch\n",
      "Epoch 10/40 Iteration: 7200 Avg. Training loss: 1.4349 0.0035 sec/batch\n",
      "error\n",
      "Epoch 10/40 Iteration: 7400 Avg. Training loss: 1.5101 0.0157 sec/batch\n",
      "Epoch 10/40 Iteration: 7600 Avg. Training loss: 1.3135 0.0159 sec/batch\n",
      "error\n",
      "Epoch 10/40 Iteration: 7800 Avg. Training loss: 1.4562 0.0156 sec/batch\n",
      "Epoch 11/40 Iteration: 8000 Avg. Training loss: 1.3009 0.0038 sec/batch\n",
      "error\n",
      "Epoch 11/40 Iteration: 8200 Avg. Training loss: 1.4690 0.0159 sec/batch\n",
      "Epoch 11/40 Iteration: 8400 Avg. Training loss: 1.4354 0.0157 sec/batch\n",
      "error\n",
      "Epoch 11/40 Iteration: 8600 Avg. Training loss: 1.3739 0.0153 sec/batch\n",
      "Epoch 12/40 Iteration: 8800 Avg. Training loss: 1.2699 0.0043 sec/batch\n",
      "error\n",
      "Epoch 12/40 Iteration: 9000 Avg. Training loss: 1.4236 0.0158 sec/batch\n",
      "Epoch 12/40 Iteration: 9200 Avg. Training loss: 1.2938 0.0157 sec/batch\n",
      "error\n",
      "Epoch 12/40 Iteration: 9400 Avg. Training loss: 1.3071 0.0153 sec/batch\n",
      "Epoch 13/40 Iteration: 9600 Avg. Training loss: 1.2650 0.0047 sec/batch\n",
      "error\n",
      "Epoch 13/40 Iteration: 9800 Avg. Training loss: 1.3989 0.0158 sec/batch\n",
      "Epoch 13/40 Iteration: 10000 Avg. Training loss: 1.4074 0.0157 sec/batch\n",
      "error\n",
      "Epoch 13/40 Iteration: 10200 Avg. Training loss: 1.3770 0.0153 sec/batch\n",
      "Epoch 14/40 Iteration: 10400 Avg. Training loss: 1.4312 0.0051 sec/batch\n",
      "error\n",
      "Epoch 14/40 Iteration: 10600 Avg. Training loss: 1.3777 0.0159 sec/batch\n",
      "Epoch 14/40 Iteration: 10800 Avg. Training loss: 1.2973 0.0157 sec/batch\n",
      "error\n",
      "Epoch 14/40 Iteration: 11000 Avg. Training loss: 1.2350 0.0153 sec/batch\n",
      "Epoch 15/40 Iteration: 11200 Avg. Training loss: 1.3477 0.0055 sec/batch\n",
      "error\n",
      "Epoch 15/40 Iteration: 11400 Avg. Training loss: 1.4263 0.0159 sec/batch\n",
      "Epoch 15/40 Iteration: 11600 Avg. Training loss: 1.3033 0.0161 sec/batch\n",
      "error\n",
      "Epoch 15/40 Iteration: 11800 Avg. Training loss: 1.3310 0.0153 sec/batch\n",
      "Epoch 16/40 Iteration: 12000 Avg. Training loss: 1.4112 0.0059 sec/batch\n",
      "error\n",
      "Epoch 16/40 Iteration: 12200 Avg. Training loss: 1.4340 0.0158 sec/batch\n",
      "Epoch 16/40 Iteration: 12400 Avg. Training loss: 1.2895 0.0156 sec/batch\n",
      "error\n",
      "Epoch 16/40 Iteration: 12600 Avg. Training loss: 1.3992 0.0153 sec/batch\n",
      "Epoch 17/40 Iteration: 12800 Avg. Training loss: 1.3596 0.0062 sec/batch\n",
      "error\n",
      "Epoch 17/40 Iteration: 13000 Avg. Training loss: 1.4184 0.0158 sec/batch\n",
      "Epoch 17/40 Iteration: 13200 Avg. Training loss: 1.3419 0.0157 sec/batch\n",
      "error\n",
      "Epoch 17/40 Iteration: 13400 Avg. Training loss: 1.2812 0.0154 sec/batch\n",
      "Epoch 18/40 Iteration: 13600 Avg. Training loss: 1.2849 0.0067 sec/batch\n",
      "error\n",
      "Epoch 18/40 Iteration: 13800 Avg. Training loss: 1.2869 0.0158 sec/batch\n",
      "Epoch 18/40 Iteration: 14000 Avg. Training loss: 1.3087 0.0160 sec/batch\n",
      "error\n",
      "Epoch 18/40 Iteration: 14200 Avg. Training loss: 1.2756 0.0153 sec/batch\n",
      "Epoch 19/40 Iteration: 14400 Avg. Training loss: 1.4024 0.0071 sec/batch\n",
      "error\n",
      "Epoch 19/40 Iteration: 14600 Avg. Training loss: 1.3466 0.0161 sec/batch\n",
      "Epoch 19/40 Iteration: 14800 Avg. Training loss: 1.2501 0.0158 sec/batch\n",
      "error\n",
      "Epoch 19/40 Iteration: 15000 Avg. Training loss: 1.3711 0.0153 sec/batch\n",
      "Epoch 20/40 Iteration: 15200 Avg. Training loss: 1.2285 0.0075 sec/batch\n",
      "error\n",
      "Epoch 20/40 Iteration: 15400 Avg. Training loss: 1.3182 0.0156 sec/batch\n",
      "Epoch 20/40 Iteration: 15600 Avg. Training loss: 1.1978 0.0158 sec/batch\n",
      "error\n",
      "Epoch 20/40 Iteration: 15800 Avg. Training loss: 1.3169 0.0152 sec/batch\n",
      "Epoch 21/40 Iteration: 16000 Avg. Training loss: 1.3991 0.0079 sec/batch\n",
      "error\n",
      "Epoch 21/40 Iteration: 16200 Avg. Training loss: 1.4160 0.0156 sec/batch\n",
      "Epoch 21/40 Iteration: 16400 Avg. Training loss: 1.2680 0.0158 sec/batch\n",
      "error\n",
      "Epoch 21/40 Iteration: 16600 Avg. Training loss: 1.3149 0.0153 sec/batch\n",
      "Epoch 22/40 Iteration: 16800 Avg. Training loss: 1.1680 0.0082 sec/batch\n",
      "error\n",
      "Epoch 22/40 Iteration: 17000 Avg. Training loss: 1.2742 0.0157 sec/batch\n",
      "Epoch 22/40 Iteration: 17200 Avg. Training loss: 1.1729 0.0157 sec/batch\n",
      "error\n",
      "Epoch 22/40 Iteration: 17400 Avg. Training loss: 1.2789 0.0152 sec/batch\n",
      "Epoch 23/40 Iteration: 17600 Avg. Training loss: 1.3264 0.0085 sec/batch\n",
      "error\n",
      "Epoch 23/40 Iteration: 17800 Avg. Training loss: 1.2721 0.0158 sec/batch\n",
      "Epoch 23/40 Iteration: 18000 Avg. Training loss: 1.1766 0.0156 sec/batch\n",
      "error\n",
      "Epoch 23/40 Iteration: 18200 Avg. Training loss: 1.3126 0.0153 sec/batch\n",
      "Epoch 24/40 Iteration: 18400 Avg. Training loss: 1.3112 0.0090 sec/batch\n",
      "error\n",
      "Epoch 24/40 Iteration: 18600 Avg. Training loss: 1.2229 0.0159 sec/batch\n",
      "Epoch 24/40 Iteration: 18800 Avg. Training loss: 1.2652 0.0158 sec/batch\n",
      "error\n",
      "Epoch 24/40 Iteration: 19000 Avg. Training loss: 1.2886 0.0153 sec/batch\n",
      "Epoch 25/40 Iteration: 19200 Avg. Training loss: 1.2296 0.0093 sec/batch\n",
      "error\n",
      "Epoch 25/40 Iteration: 19400 Avg. Training loss: 1.2576 0.0158 sec/batch\n",
      "Epoch 25/40 Iteration: 19600 Avg. Training loss: 1.2228 0.0156 sec/batch\n",
      "error\n",
      "Epoch 25/40 Iteration: 19800 Avg. Training loss: 1.2977 0.0155 sec/batch\n",
      "Epoch 26/40 Iteration: 20000 Avg. Training loss: 1.3057 0.0098 sec/batch\n",
      "error\n",
      "Epoch 26/40 Iteration: 20200 Avg. Training loss: 1.3677 0.0159 sec/batch\n",
      "Epoch 26/40 Iteration: 20400 Avg. Training loss: 1.2412 0.0156 sec/batch\n",
      "error\n",
      "Epoch 26/40 Iteration: 20600 Avg. Training loss: 1.2936 0.0154 sec/batch\n",
      "Epoch 27/40 Iteration: 20800 Avg. Training loss: 1.3572 0.0102 sec/batch\n",
      "error\n",
      "Epoch 27/40 Iteration: 21000 Avg. Training loss: 1.2085 0.0158 sec/batch\n",
      "Epoch 27/40 Iteration: 21200 Avg. Training loss: 1.2033 0.0154 sec/batch\n",
      "error\n",
      "Epoch 27/40 Iteration: 21400 Avg. Training loss: 1.2693 0.0153 sec/batch\n",
      "Epoch 28/40 Iteration: 21600 Avg. Training loss: 1.2663 0.0105 sec/batch\n",
      "error\n",
      "Epoch 28/40 Iteration: 21800 Avg. Training loss: 1.1293 0.0164 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 Iteration: 22000 Avg. Training loss: 1.0830 0.0157 sec/batch\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "Tdl.train_s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
